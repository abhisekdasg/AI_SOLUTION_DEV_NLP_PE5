{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bad6c9f",
   "metadata": {},
   "source": [
    "# Day 5 NLP 's practice Assignments:\n",
    "    1.Build a Question answering model with Transformers from huggingface.\n",
    "    2.Build a Translation model using transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799140b7",
   "metadata": {},
   "source": [
    "#    1.Build a Question answering model with Transformers from huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2a802db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e25f890961349dab624b7f60ad4ac87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhisek Das\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Abhisek Das\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9784499e42574a5b820f40e0c6094dfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ab6a6516249470ab517f86d985f3c7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e6e5758063640b59a6804aeeeeb0ea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da3bcaa10a9342cfbb8df228908894f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What does Hugging Face specialize in?\n",
      "Answer: Natural Language Processing models\n",
      "Score: 0.9538402557373047\n",
      "Start index: 46\n",
      "End index: 80\n",
      "\n",
      "=====\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who founded Hugging Face?\n",
      "Answer: Natural Language Processing models\n",
      "Score: 0.23899216949939728\n",
      "Start index: 46\n",
      "End index: 80\n",
      "\n",
      "=====\n",
      "\n",
      "Question: When was Hugging Face established?\n",
      "Answer: Hugging Face is a company that specializes in Natural Language Processing models\n",
      "Score: 0.18240849673748016\n",
      "Start index: 0\n",
      "End index: 80\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def run_qa_model(context, question):\n",
    "    # Load the question answering model\n",
    "    qa_model = pipeline(\"question-answering\")\n",
    "\n",
    "    # Get the answer using the question answering model\n",
    "    answer = qa_model(question=question, context=context)\n",
    "\n",
    "    # Print the answer\n",
    "    print(\"Question:\", question)\n",
    "    print(\"Answer:\", answer['answer'])\n",
    "    print(\"Score:\", answer['score'])\n",
    "    print(\"Start index:\", answer['start'])\n",
    "    print(\"End index:\", answer['end'])\n",
    "\n",
    "\n",
    "context = \"Hugging Face is a company that specializes in Natural Language Processing models.\"\n",
    "\n",
    "# Asking a series of questions\n",
    "questions = [\n",
    "    \"What does Hugging Face specialize in?\",\n",
    "    \"Who founded Hugging Face?\",\n",
    "    \"When was Hugging Face established?\"\n",
    "]\n",
    "\n",
    "# Now Running the QA model for each question\n",
    "for question in questions:\n",
    "    print(\"\\n=====\\n\")\n",
    "    run_qa_model(context, question)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c1bae2",
   "metadata": {},
   "source": [
    "# 2.Build a Translation model using transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ac09d4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "476bdd29b27e415abde649e20ecb7f5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.42k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e69c8d80a5043e681ff69169839b528",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/301M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56a234389ef24aae82c287cde6661351",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81dc5ae7582d4fbe97c857026cbe31fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c48221f1a7a4448a952e6e9ee2ea5005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "source.spm:   0%|          | 0.00/778k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a09b8f99248b4133b82331772ac998ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "target.spm:   0%|          | 0.00/802k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a3b65f967a14028bf4a0788a78f19ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.34M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Text (en): Hello, how are you?\n",
      "Translated Text (fr): Bonjour, comment allez-vous ?\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def run_translation_model(text, source_lang, target_lang):\n",
    "    # Load the translation model\n",
    "    translator = pipeline(\"translation\", model=f\"Helsinki-NLP/opus-mt-{source_lang}-{target_lang}\")\n",
    "\n",
    "    # Translate the text\n",
    "    translated_text = translator(text, max_length=500)[0]['translation_text']\n",
    "\n",
    "    # Print the translated text\n",
    "    print(f\"Source Text ({source_lang}): {text}\")\n",
    "    print(f\"Translated Text ({target_lang}): {translated_text}\")\n",
    "\n",
    "# Example English text\n",
    "english_text = \"Hello, how are you?\"\n",
    "\n",
    "# Translate from English to French\n",
    "run_translation_model(english_text, source_lang=\"en\", target_lang=\"fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bccdfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae4968a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
